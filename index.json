[{"content":"       Play with it 😎 Write a message, we will encrypt it with symbols (here emoji 😉) by replacing each alphabet letter with symbols. Then our algorithm will try to reconstruct the original message.\nYour message Bonjour, on va essayer de decrypter ce message! mais je ne suis pas certain que ca va marcher  ENCRYPT 🔑  Encrypted message 🔒   Decrypted message    DECRYPT  STOP         What\u0026rsquo;s the problem 🙋 Between 1968 and 1970, a serial killer nicknamed the Zodiac Killer murdered at least 5 people in the San Francisco area, and he is probably responsible for several dozen other unconfirmed cases. He was known for taunting the authorities by sending letters to local newspapers that contained such coded messages. The killer was never identified, and the case has been officially open for over 50 years.\nThe encryption of the message was done by changing the characters of the alphabet by symbols in order to make the message incomprehensible. So our goal is to find these correspondences to the rigth character in order to reconstruct the real message. In the next section we will see that we can solve this problem using optimization.\nI first get in touch we this problem while I was watching a French science channel video on the topic (ScienceEtonnante). I just try to reproduce and make it online so people can challenge the algorithm.\nSolve it as combinatorial optimization problem 🧮 In reality we are dealing with a combinatorial optimization problem. The objective is to assign the symbols to the letters of the alphabet to reconstitute the message. For that we will need a score that will allow us to evaluate our solution. Here our score function will be the plausibility that the reconstrcution we obtain is plausible in the language we are considering (Here English). Let\u0026rsquo;s consider for example these two proposals for decoding a message:\n  Proposal 1\n qunduor, un fa ettager je jecrgyser ce pettave! pait de ne toit yat cersain boe ca fa parcler\n   Proposal 2\n qunduor, un fa ettager je jecrgyser ce pettave! pait de ne toit yat cersain boe ca fa parcler\n   Proposal 1 is more plausible because it contains several letter sequences that are found in English. That\u0026rsquo;s it! It is with this notion of plausibility that we will build our score function.\nPlausibility as objective function Our objective function will use the bi-gram probabilities. The matrix below represents the character chaining probabilities that we obtain by parsing the book.\n   The solution Once we have our function, all we have to do is apply a stochastic optimization algorithm and we are done. Here we will apply the simulated annealing algorithm.\n","permalink":"https://kanvaly.io/posts/decryptage/","summary":"Play with it 😎 Write a message, we will encrypt it with symbols (here emoji 😉) by replacing each alphabet letter with symbols. Then our algorithm will try to reconstruct the original message.\nYour message Bonjour, on va essayer de decrypter ce message! mais je ne suis pas certain que ca va marcher  ENCRYPT 🔑  Encrypted message 🔒   Decrypted message    DECRYPT  STOP         What\u0026rsquo;s the problem 🙋 Between 1968 and 1970, a serial killer nicknamed the Zodiac Killer murdered at least 5 people in the San Francisco area, and he is probably responsible for several dozen other unconfirmed cases.","title":"Decipher messages 🔐"},{"content":"      Greedy 🤤  Simulated annealing 🌡️  Genetic Algorithm 🧬   Initial sensor placement :  random  grid    Run  Stop          ","permalink":"https://kanvaly.io/posts/sensor-placement/","summary":"      Greedy 🤤  Simulated annealing 🌡️  Genetic Algorithm 🧬   Initial sensor placement :  random  grid    Run  Stop          ","title":"Sensor placement 🎛️"},{"content":"We build an agent that master Quoridor Board Game. Quoridor is not so famous game and there is not a lot of research about it. It has a state-space complexity similar to Chess with a higher game-tree complexity. We code this game like AlphaGo Zero from Deepmind, which learns to play the game by self playing without human games data.\n","permalink":"https://kanvaly.io/projects/alpha-quoridor/","summary":"We build an agent that master Quoridor Board Game. Quoridor is not so famous game and there is not a lot of research about it. It has a state-space complexity similar to Chess with a higher game-tree complexity. We code this game like AlphaGo Zero from Deepmind, which learns to play the game by self playing without human games data.","title":"Alpha-Quoridor 🕹️"},{"content":"In this work , we were trying to predict credit default risk. We were dealing with historical credit clients informations and willing to predict whether or not a client will reimbursed his credit . This is a standard supervised classification task . First of all , we analyze and clean the data . Then , extract relevelant pattern. Then train models and optimize the most promising . Finally, we use stacking to build a strong predictor.\n","permalink":"https://kanvaly.io/projects/credit-default-prediction/","summary":"In this work , we were trying to predict credit default risk. We were dealing with historical credit clients informations and willing to predict whether or not a client will reimbursed his credit . This is a standard supervised classification task . First of all , we analyze and clean the data . Then , extract relevelant pattern. Then train models and optimize the most promising . Finally, we use stacking to build a strong predictor.","title":"Credit default prediction 💰 "},{"content":"Our task was on predicting links between pages in a subgraph of the French webgraph. The webgraph is a directed graph G(V, E) whose vertices correspond to the pages of the French web. A directed edge connects page U to page V if there exists a hyperlink on page ​ U pointing to page ​ V . From the original subgraph, edges have been deleted at random​ . Given a set of candidate edges, our job was to predict which ones appeared in the original subgraph. Each node is associated with a text file extracted from the html of the corresponding webpage.\n","permalink":"https://kanvaly.io/projects/link-prediction/","summary":"Our task was on predicting links between pages in a subgraph of the French webgraph. The webgraph is a directed graph G(V, E) whose vertices correspond to the pages of the French web. A directed edge connects page U to page V if there exists a hyperlink on page ​ U pointing to page ​ V . From the original subgraph, edges have been deleted at random​ . Given a set of candidate edges, our job was to predict which ones appeared in the original subgraph.","title":"Link prediction 🔗"},{"content":"One approaches for the simulation of dynamic systems in computer graphics are force based. Internal and external forces are accumulated from which accelerations are computed based on Newton’s second law of motion. A time integration method is then used to update the velocities and finally the positions of the object. A few simulation methods (most rigid body simulators) use impulse based dynamics and directly manipulate velocities. The paper I studyed present an approach which omits the velocity layer as well and immediately works on the positions. This methods can simulate multiple object types such as gases, liquids, deformable solids, rigid bodies and cloth in a single piece of software by using constraint solver and position-based dynamics (PBD).\n","permalink":"https://kanvaly.io/projects/position-based-dynamics/","summary":"One approaches for the simulation of dynamic systems in computer graphics are force based. Internal and external forces are accumulated from which accelerations are computed based on Newton’s second law of motion. A time integration method is then used to update the velocities and finally the positions of the object. A few simulation methods (most rigid body simulators) use impulse based dynamics and directly manipulate velocities. The paper I studyed present an approach which omits the velocity layer as well and immediately works on the positions.","title":"Position Based dynamics 🤾‍♂️"},{"content":"Our goal was to firstly build a multiple object tracker by different methods of detection and tracking. Then choose an application domain. We chose to work on surveillance and security. The user can segment region of interest in a video scene and any new object motion, position and time spend there. Then after a certain amount of time, we start identification (if it is a human or a car) in an asynchronous way and launch alarm given the result of identification.\n","permalink":"https://kanvaly.io/projects/object-tracking/","summary":"Our goal was to firstly build a multiple object tracker by different methods of detection and tracking. Then choose an application domain. We chose to work on surveillance and security. The user can segment region of interest in a video scene and any new object motion, position and time spend there. Then after a certain amount of time, we start identification (if it is a human or a car) in an asynchronous way and launch alarm given the result of identification.","title":"Object Tracking"},{"content":"I choose to work on the problem of sketch-based retrieval for this semester, where the goal is to find relevant 3D models in a large collection. The input is a rough 2D sketch, rather than a complete 3D model. This problem is challenging and will alows me to use very well-developed techniques from image analysis. My goal will be to read and implement a recent paper, Sketch-Based Shape Retrieval, and test it on any dataset, such as the one described here.\n","permalink":"https://kanvaly.io/projects/sketch-based-shape-retrieval/","summary":"I choose to work on the problem of sketch-based retrieval for this semester, where the goal is to find relevant 3D models in a large collection. The input is a rough 2D sketch, rather than a complete 3D model. This problem is challenging and will alows me to use very well-developed techniques from image analysis. My goal will be to read and implement a recent paper, Sketch-Based Shape Retrieval, and test it on any dataset, such as the one described here.","title":"Sketch-based shape retrieval"},{"content":"For tasks such as face recognition, facial landmarks detection or gaze estimation state-of-the-art is often achieved using deep neural networks. One major drawback of this technology is that it requires a huge amount of training data. Fortunately, there are huge datasets publicly available for RGB image so when your application is based on RGB image there is no difficulty in gathering data. However, for the application of those networks on infrared (IR) imagery there are often not enough annotated data available (if not none). To tackle this issue I study how to translate a RGB face image to a physically realistic IR face image using Generative Adversarial Networks (GAN). It enable to translate annotated RGB dataset to IR and thus create a large training dataset enabling us to create state-of-the-art networks on IR data.\n","permalink":"https://kanvaly.io/projects/intership-rgb-to-infrared-translation-using-gan/","summary":"For tasks such as face recognition, facial landmarks detection or gaze estimation state-of-the-art is often achieved using deep neural networks. One major drawback of this technology is that it requires a huge amount of training data. Fortunately, there are huge datasets publicly available for RGB image so when your application is based on RGB image there is no difficulty in gathering data. However, for the application of those networks on infrared (IR) imagery there are often not enough annotated data available (if not none).","title":"Intership - RGB to Infrared translation using GAN"},{"content":"Hello friends! Today equipped with our Bayesian magnifying glass 🔎 we are going on a Safari to count animals. Inspired by Andrew M. Webb tweet I tryied to reproduce what he made. I will explain to you how to count the number of animals by using Mark and recapture method and Bayesian Inference. First let\u0026rsquo;s take a look at the result, then, explain how we get to it.\nResult 🔥 Below you can click on the button Capture many times to capture particle, mark and release them. We then use what we observe to update our belief on the number of animal. The red line is the correct value.\n          Capture     Mark and Recapture 🪤 Mark and recapture is a method commonly used in ecology to estimate an animal population\u0026rsquo;s size where it is impractical to count every individual. So what do we do ? we catch some animals, mark them, release them, and catch some again. In the second capture, there will be some marked animals and some unmarked (i.e., not seen before). The ratio gives you some information about the population size.\nWe\u0026rsquo;ll make these simplifying modeling assumptions:\n  The population size is constant. Animals don\u0026rsquo;t leave or join the population between capture events. Every animal has an equal probability of being captured, and this probability is independent between capture events. The total number of animals captured in a given capture event does not depend on the total population size (apart from being upper-bounded by it). This assumption is actually false the animation above, but is often true in mark-and-recapture field work. Note, if the total number of animals captured at each stage did depend on the population size, the total number observed would give us further information about the population size.   Belief Update 🤔 We will use the information of each capture to update our belief on the population size. Bayes Theorem is the main part of this update. Let\u0026rsquo;s explain with a simple example. Imagine we thought that the population size is $pop=10$. we make our capture, and get 11 animals in our first try. bayesian belief update will update our belief using this rules: belief on $pop=10$ is multiplied by the likelihood of catching 11 animals if the $pop=10$. For our example the likelihood will be very low because capturing 11 if the population is 10 it is very unlikely (here impossible probability=0). This is exactly what we do for a range of prior population size. Then we normalize over all resulting value.\nSo, Bayesian Update is made as follow:\n  Specify a prior distribution over hypotheses, in the absence of observational data. For a given hypothesis and observation, compute the likelihood, i.e., the probability of having made that observation. Re-normalize the posterior distribution, so that it sums to 1.    The maths behind 🧮 (don't be afraid it's simple) $\\small{\\mathbb{P}(H \\mid O) \\propto \\mathbb{P}(O \\mid H) \\mathbb{P}(O)}$  If we let O stand for an observation, and H for a hypothesis. In our case it will look like :\n$\\scriptsize{\\mathbb{P}(X=pop \\mid cap_{new}=k, cap_{already}=n) \\propto \\underbrace{\\mathbb{P}(cap_{new}=k \\mid X=k, cap_{already}=n)}_{\\text{likelihood}} \\underbrace{\\mathbb{P}(X=pop \\mid cap_{already}=n)}_{\\text{prior}}}$  Where $cap_{new}$ is the number of new animals and $cap_{already}$ stand for the number of animals that we have already capture.\n$\\small{\\mathbb{P}(cap_{new}=k \\mid X=k, cap_{already}=n)}$ follow a hypergeometric distribution.\n","permalink":"https://kanvaly.io/posts/capture-recapture/","summary":"Hello friends! Today equipped with our Bayesian magnifying glass 🔎 we are going on a Safari to count animals. Inspired by Andrew M. Webb tweet I tryied to reproduce what he made. I will explain to you how to count the number of animals by using Mark and recapture method and Bayesian Inference. First let\u0026rsquo;s take a look at the result, then, explain how we get to it.\nResult 🔥 Below you can click on the button Capture many times to capture particle, mark and release them.","title":"Mark and Recapture 🦒"},{"content":"Our project is part of an initiative jointly initiated by ENSTA Bretagne and EDF concerning the dam on Lake Guerlédan, which aims to check the state of the evacuation grids set up by EDF to evacuate water from the cooling circuits of nuclear power plants into the nearby river.\nThe drone we want to design and build will have to be able not only to fly, but also to immerse itself in order to evolve in an aquatic environment, in order to access dangerous areas for divers. It will have to maintain its stability in currents that may exist in all watercourses or around a dam. An autonomous travel procedure is also envisaged.\n","permalink":"https://kanvaly.io/projects/flying-fish/","summary":"Our project is part of an initiative jointly initiated by ENSTA Bretagne and EDF concerning the dam on Lake Guerlédan, which aims to check the state of the evacuation grids set up by EDF to evacuate water from the cooling circuits of nuclear power plants into the nearby river.\nThe drone we want to design and build will have to be able not only to fly, but also to immerse itself in order to evolve in an aquatic environment, in order to access dangerous areas for divers.","title":"Flying Fish"},{"content":"A debugger is tools that helps you to solve problems that are not understood in your scripts.\nIt allows you to execute a program step by step, to study the execution of the code in real-time and to see the value of the variables at a precise place.\nYou can debug your script as you wish; often a print is enough but in more complex cases you can use more advanced tool such as ipdb which is a combination of ipython and pdb\n📥 Installing ipdb Let\u0026rsquo;s go through pip :\npip install ipdb 🐞 Implement ipdb in your program You just need to add the following line at the place where you want to debug your code:\nimport ipdb; ipdb.set_trace() 💡 Exemple Let\u0026rsquo;s add set_trace to our code like this:\nclass LoginView(): template_name = \u0026#39;front/index.html\u0026#39; def post(self, request, **kwargs): import ipdb; ipdb.set_trace() username = request.get(\u0026#39;username\u0026#39;, False) password = request.get(\u0026#39;password\u0026#39;, False) return (username, password) Then let\u0026rsquo;s run our code (in our case it is a Django script)\nWe see that the script has stopped and this is displayed in our console:\n----\u0026gt; 9 username = request.get(\u0026#39;username\u0026#39;, False) 10 password = request.get(\u0026#39;password\u0026#39;, False) ipdb\u0026gt; At this point you can do whatever you want to do. for instance:\n  look/change variables values\nipdb\u0026gt; self.template_name = \u0026#34;front/error.html\u0026#34; ipdb\u0026gt; print(self.template_name) \u0026#34;front/error.html\u0026#34;   n $\\rightarrow$ execute the current line and move to the next line\nipdb\u0026gt; n 9 username = request.get(\u0026#39;username\u0026#39;, False) ---\u0026gt; 10 password = request.get(\u0026#39;password\u0026#39;, False) 11 ipdb\u0026gt;   q $\\rightarrow$ quit wildly\n  c $\\rightarrow$ continue the execution of the program until its end\n  s $\\rightarrow$ enter the function of the current line\n  r $\\rightarrow$ execute a return\n  🤌 My usage This is one of the most powerful to that I discover in python (for real). I see lot of people placing many print then running and after seeing that there is nothing wrong with what they print they print another variable. with ipdb the code stop and you can watch whatever you want then continue the running.\nIn ML model I use it with pytorch in forward methods to reorganize my matrix multiplication when I am confused. Sometime I put it in try catch like this example:\ntry: # code except: import ipdb; ipdb.set_trace() ","permalink":"https://kanvaly.io/posts/ipdb-debug-your-python-code-easily/","summary":"A debugger is tools that helps you to solve problems that are not understood in your scripts.\nIt allows you to execute a program step by step, to study the execution of the code in real-time and to see the value of the variables at a precise place.\nYou can debug your script as you wish; often a print is enough but in more complex cases you can use more advanced tool such as ipdb which is a combination of ipython and pdb","title":"ipdb - debug your python code easily"},{"content":"Our Project consists of modeling the landscape of an oasis on the banks of the Nile. The landscape represents a desert in the vicinity of a large river and has several static and animated elements that will be detailed later. The project consists of two main stages: first, we modeled the desert, which gave us a good starting point. Then, we added more and more complex extensions based on some models seen in class.\n","permalink":"https://kanvaly.io/projects/3d-sahara-desert/","summary":"Our Project consists of modeling the landscape of an oasis on the banks of the Nile. The landscape represents a desert in the vicinity of a large river and has several static and animated elements that will be detailed later. The project consists of two main stages: first, we modeled the desert, which gave us a good starting point. Then, we added more and more complex extensions based on some models seen in class.","title":"3D Sahara desert 🏜️"},{"content":"As part of the mini-project of my Modal IoT I decided to transform my room into a smart one. This way, I will be able to control the state of the light, open my door or generate LED animations with my voice, my phone or by voice command. So I made different objects that allowed me to make this possible.\n","permalink":"https://kanvaly.io/projects/smart-home-control/","summary":"As part of the mini-project of my Modal IoT I decided to transform my room into a smart one. This way, I will be able to control the state of the light, open my door or generate LED animations with my voice, my phone or by voice command. So I made different objects that allowed me to make this possible.","title":"Smart home control"},{"content":"    Today we are going to talk compression and biology. I will present you a new framework to see machine learning which is that learning is compression. We will explore together how we can solve a real-world problem by only zipping content. Yes! you heard me, we will zip data like what we do when we send an email. This will allow us to classify proteins.\n🧠 Learning is compression  Comprehension is compression. (Gregory Chaitin, 2004)\n I don\u0026rsquo;t know if you ever heard these statements. Let\u0026rsquo;s explain what they mean by considering clustering. In this task, we consider a dataset $X$ of $N$ points $x_j$. Each $x_j$ is supposed to belong to a class labelled as $y_j$ (e.g. blue/red/dark). The problem is to learn which label $y_j$ taken from the set of labels $Y$ corresponds to $x_j$.\nOur clustering model would deduce the label of all points from those of its K cluster center {$p_k$}, in the hope that: $f(x_i)=y_i$\nThe decision function f just returns the label of the closest center. For instance, if the point is closer to center 3 (labeled \u0026ldquo;blue\u0026rdquo;) than to any other center, the decision function will return \u0026ldquo;blue\u0026rdquo;.\nNow we just have to remember the centers and their label to retrieve all labels, hence the compression 😎.\n🤯 Use zip to measure distance I want to introduce you a new distance metric call Normalized Compression Distance (NCD). NCD is a way of measuring the similarity between two objects, be it two documents, two letters, two emails, two music scores, two languages, two programs, two pictures, two systems, two genomes, to name a few. It measure how hard it is to compress $seq_1$ alone than compressing it concatenated with $seq_2$. We will measure how close two protein sequences.\n$NCD_Z(x,y)=\\\\frac{Z(xy)-min(Z(x),Z(y))}{max(Z(x),Z(y))}$  $Z$ is a compressor like zip or gzip. $Z(x)$ measure number of bit it take to compress $x$ alone. $Z(xy)$ measure the number of bit used to compressed $x$ and $y$ together.\nimport zlib # Define our three sequence seq1 = \u0026#34;ABCDEFGHIJK\u0026#34;.encode(\u0026#39;latin-1\u0026#39;) seq2 = \u0026#34;AABBCCDDEEFF\u0026#34;.encode(\u0026#39;latin-1\u0026#39;) # compressing alone z1 = len(zlib.compress(seq1)) z2 = len(zlib.compress(seq2)) print(f\u0026#39;Compressed size | Z(seq1)={z1}, Z(seq2)={z2}\u0026#39;) # co-compressing z12 = len(zlib.compress(seq1 + seq2)) print(f\u0026#39;Compressed size | Z(seq1+seq2)={z12}\u0026#39;) #NCD ncd = (z12 - min(z1,z2)) / max(z1,z2) print(f\u0026#39;NCD(seq1, seq2) = {ncd}\u0026#39;) Compressed size | Z(seq1)=19, Z(seq2)=20 Compressed size | Z(seq1+seq2)=31 NCD(seq1, seq2) = 0.6 🧬 Application to protein classification Proteins can be made from 20 different kinds of amino acids, and the structure and function of each protein are determined by the kinds of amino acids used to make it and how they are arranged. Understanding this relationship between amino acid sequence and protein function is a long-standing problem in molecular biology with far-reaching scientific implications. On application of ML in biology is to classify the protein’s amino acid sequence to one of the protein family accession. In other words, the task is: given the amino acid sequence of the protein domain, predict which class it belongs to.\nThis is how an aligned protein sequence looks like: ....HWLQMRDSMNTYNNMVNRCFATCI...........RS.F....QEKKVNAEE\nWe use this compression distance to look how protein within a family same are far from each other (intra family variation):\n  And how far they are from sequences coming from another family (extra family variation) :\n   As we see sequence from the same family are very class while those from other family are far. We obtain this cool result using only zip. Now we can train a kNN with this distance and classify proteins sequences.\n","permalink":"https://kanvaly.io/posts/protein/","summary":"Today we are going to talk compression and biology. I will present you a new framework to see machine learning which is that learning is compression. We will explore together how we can solve a real-world problem by only zipping content. Yes! you heard me, we will zip data like what we do when we send an email. This will allow us to classify proteins.\n🧠 Learning is compression  Comprehension is compression.","title":"Normalized Compression Distance"},{"content":"This page is dedicated to resources and blogs that I find interesting.\nBlog  Vincent D. Warmerdam - koaning Andrej Karpathy (Sr. Director of AI at Tesla) calmcode  AI  Twitter AI tweets Machine Learning subreddit  ","permalink":"https://kanvaly.io/connector/","summary":"This page is dedicated to resources and blogs that I find interesting.\nBlog  Vincent D. Warmerdam - koaning Andrej Karpathy (Sr. Director of AI at Tesla) calmcode  AI  Twitter AI tweets Machine Learning subreddit  ","title":"Connector"},{"content":"Papers Review  ","permalink":"https://kanvaly.io/papers/","summary":"Papers Review  ","title":"paper"}]